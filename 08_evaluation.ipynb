{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b35536f6-166c-4b89-8136-96417db5be30",
      "metadata": {
        "id": "b35536f6-166c-4b89-8136-96417db5be30"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
      "metadata": {
        "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
      },
      "source": [
        "<br>\n",
        "\n",
        "# <font color=\"#76b900\">**Notebook 8 [Assessment]:** RAG Evaluation</font>\n",
        "\n",
        "<br>\n",
        "\n",
        "Welcome to the last notebook of the course! In the previous notebook, you integrated a vector store solution into a RAG pipeline! In this notebook, you will take that same pipeline and evaluate it using numerical RAG evaluation techniques incorporating LLM-as-a-Judge metrics!\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Learning Objectives:**\n",
        "\n",
        "- Learn how to integrate the techniques from prior notebooks to numerically approximate the goodness of your RAG pipeline.\n",
        "\n",
        "- **Final Exercice**: ***By working through this notebook in the Course Environment,* you will be able to submit the coding component of the course!**\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Questions To Think About:**\n",
        "\n",
        "- As you go along, remember what our metrics actually represent. Should our pipeline pass these objectives? Is our judge LLM sufficient for evaluating the pipeline? Does a particular metric even matter for our use case?\n",
        "- If we left the vectorstore-as-a-memory component in our chain, do you think it would still pass the evaluation? Additionally, is the evaluation useful for assessing vectorstore-as-a-memory performance?\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Notebook Source:**\n",
        "\n",
        "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Environment Setup:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w_A3rZOrIeQD",
      "metadata": {
        "id": "w_A3rZOrIeQD"
      },
      "outputs": [],
      "source": [
        "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
        "# %pip install -q arxiv pymupdf faiss-cpu ragas\n",
        "\n",
        "## If you encounter a typing-extensions issue, restart your runtime and try again\n",
        "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "# ChatNVIDIA.get_available_models()\n",
        "\n",
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "norm_style = Style(bold=True)\n",
        "pprint = partial(console.print, style=base_style)\n",
        "pprint2 = partial(console.print, style=norm_style)\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models(base_url=\"http://llm_client:9000/v1\")\n",
        "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zEgV11oZmJGg",
      "metadata": {
        "id": "zEgV11oZmJGg"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 1:** Pre-Release Evaluation\n",
        "\n",
        "In our previous notebook, we successfully combined several concepts to create a document chatbot with the aim of responsive and informative interactions. However, the diversity of user interactions necessitates comprehensive testing to truly understand the chatbot's performance. Thorough testing in varied scenarios is crucial to ensure that the system is not only robust and versatile but also aligns with user and provider expectations.\n",
        "\n",
        "After defining your chatbot's roles and implementing the necessary features, evaluating it becomes a multi-stage process:\n",
        "\n",
        "- **Typical Use Inspection:** Start by testing scenarios most relevant to your use case. See if your chatbot can reliably navigate discussions with limited human intervention.\n",
        "\n",
        "    - Additionally, identify limitations or compartments that should be redirected to a human for inspection/supervision (i.e., human swap-in to confirm transactions or perform sensitive navigation) and implement those options.\n",
        "\n",
        "- **Edge Case Inspection:** Explore the boundaries of typical use, identifying how the chatbot handles less common but plausible scenarios.\n",
        "\n",
        "    - Before any public release, assess critical boundary conditions that could pose liability risks, such as the potential generation of inappropriate content.\n",
        "\n",
        "    - Implement well-tested guardrails on all outputs (and possibly inputs) to limit undesired interactions and redirect users into predictable conversation flows.\n",
        "\n",
        "- **Progressive Rollout:** Rolling out your model to a limited audience (first internal, then [A/B](https://en.wikipedia.org/wiki/A/B_testing)) and implement analytics features like usage analytics dashboards and feedback avenues (flag/like/dislike/etc).\n",
        "\n",
        "Of these three steps, the first two can be done by a small team or an individual and should be iterated on as part of the development process. Unfortunately, this needs to be done frequently and can be prone to human error. **Luckily for us, LLMs can be used to help out with LLM-as-a-Judge formulations!**\n",
        "\n",
        "*(Yeah, this probably isn't surprising by now. LLMs being strong is why this course is here...).*\n",
        "\n",
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 2:** LLM-as-a-Judge Formulation\n",
        "\n",
        "In the realm of conversational AI, using LLMs as evaluators or 'judges' has emerged as a useful approach for configurable automatic testing of natural language task performance:\n",
        "\n",
        "- An LLM can simulate a range of interaction scenarios and generate synthetic data, allowing an evaluation developer to generate targeted inputs to eliciting a range of behaviors from your chatbot.\n",
        "\n",
        "- The chatbot's correspondence/retrieval on the synthetic data can be evaluated or parsed by an LLM and a consistent output format such as \"Pass\"/\"Fail\", similarity, or extraction can be enforced.\n",
        "\n",
        "- Many such results can be aggregated and a metric can be derived which explains something like \"% of passing evaluations\", \"average number of relevant details from the sources\", \"average cosine similarity\", etc.\n",
        "\n",
        "This idea of using LLMs to test out and quantify chatbot quality, known as [**\"LLM-as-a-Judge,\"**](https://arxiv.org/abs/2306.05685) allows for easy test specifications that align closely with human judgment and can be fine-tuned and replicated at scale.\n",
        "\n",
        "**There are several popular frameworks for off-the-shelf judge formulations including:**\n",
        "- [**RAGAs (short for RAG Assessment)**](https://docs.ragas.io/en/stable/), which offers a suite of great starting points for your own evaluation efforts.\n",
        "- [**LangChain Evaluators**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/), which are similar first-party options with many implicitly-constructible agents.\n",
        "\n",
        "Instead of using the chains as-is, we will instead expand on the ideas and evaluate our system with a more custom solution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fDDNaBA9N3XM",
      "metadata": {
        "id": "fDDNaBA9N3XM"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 3: [Assessment Prep]** Pairwise Evaluator\n",
        "\n",
        "The following exercise will flesh out a custom implementation of a simplified [LangChain Pairwise String Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/comparison/pairwise_string/).\n",
        "\n",
        "**To prepare for our RAG chain evaluation, we will need to:**\n",
        "\n",
        "- Pull in our document index (the one we saved in the previous notebook).\n",
        "- Recreate our RAG pipeline of choice.\n",
        "\n",
        "**We will specifically be implementing a judge formulation with the following steps:**\n",
        "\n",
        "- Sample the RAG agent document pool to find two document chunks.\n",
        "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
        "- Use the RAG agent to generate its own answer.\n",
        "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
        "\n",
        "**The chain should be a simple but powerful process that tests for the following objective:**\n",
        "\n",
        "> ***Does my RAG chain outperform a narrow chatbot with limited document access.***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**This will be the system used for the final evaluation!** To see how this system is integrated into the autograder, please check out the implementation in [`frontend/server_app.py`](frontend/server_app.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bh8jaOqak0f",
      "metadata": {
        "id": "1bh8jaOqak0f"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Task 1:** Pull In Your Document Retrieval Index\n",
        "\n",
        "For this exercise, you will pull in the `docstore_index` file you created as part of your earlier notebook. The following cell should be able to load in the store as-is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tlE7a2lseLOy",
      "metadata": {
        "id": "tlE7a2lseLOy",
        "outputId": "31c06847-fa6e-44c3-ae6d-5d92478972dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "docstore_index/\n",
            "docstore_index/index.pkl\n",
            "docstore_index/index.faiss\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Constructed aggregate docstore with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">238</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> chunks</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mConstructed aggregate docstore with \u001b[0m\u001b[1;36m238\u001b[0m\u001b[1;38;2;118;185;0m chunks\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sample Chunk:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSample Chunk:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paper: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
            "\n",
            "Summary: Large pre-trained language models have been shown to store factual knowledge\n",
            "in their parameters, and achieve state-of-the-art results when fine-tuned on\n",
            "downstream NLP tasks. However, their ability to access and precisely manipulate\n",
            "knowledge is still limited, and hence on knowledge-intensive tasks, their\n",
            "performance lags behind task-specific architectures. Additionally, providing\n",
            "provenance for their decisions and updating their world knowledge remain open\n",
            "research problems. Pre-trained models with a differentiable access mechanism to\n",
            "explicit non-parametric memory can overcome this issue, but have so far been\n",
            "only investigated for extractive downstream tasks. We explore a general-purpose\n",
            "fine-tuning recipe for retrieval-augmented generation (RAG) -- models which\n",
            "combine pre-trained parametric and non-parametric memory for language\n",
            "generation. We introduce RAG models where the parametric memory is a\n",
            "pre-trained seq2seq model and the non-parametric memory is a dense vector index\n",
            "of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\n",
            "formulations, one which conditions on the same retrieved passages across the\n",
            "whole generated sequence, the other can use different passages per token. We\n",
            "fine-tune and evaluate our models on a wide range of knowledge-intensive NLP\n",
            "tasks and set the state-of-the-art on three open domain QA tasks, outperforming\n",
            "parametric seq2seq models and task-specific retrieve-and-extract architectures.\n",
            "For language generation tasks, we find that RAG models generate more specific,\n",
            "diverse and factual language than a state-of-the-art parametric-only seq2seq\n",
            "baseline.\n",
            "\n",
            "Page Body: .S.\\nRAG-T It\\u2019s the only U.S. state named for a U.S. president\\nRAG-S It\\u2019s the state where you\\u2019ll \\ufb01nd Mount Rainier National Park\\nThe Divine\\nComedy\\nBART\\n*This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\\nRAG-T Dante\\u2019s \\\"Inferno\\\" is the \\ufb01rst part of this epic poem\\nRAG-S This 14th century work is divided into 3 sections: \\\"Inferno\\\", \\\"Purgatorio\\\" & \\\"Paradiso\\\"\\nFor 2-way classi\\ufb01cation, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\\nevidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved\\nby RAG and gold evidence annotations\n"
          ]
        }
      ],
      "source": [
        "## Make sure you have docstore_index.tgz in your working directory\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# embedder = NVIDIAEmbeddings(model=\"nvidia/embed-qa-4\", truncate=\"END\")\n",
        "\n",
        "!tar xzvf docstore_index.tgz\n",
        "docstore = FAISS.load_local(\"docstore_index\", embedder, allow_dangerous_deserialization=True)\n",
        "docs = list(docstore.docstore._dict.values())\n",
        "\n",
        "def format_chunk(doc):\n",
        "    return (\n",
        "        f\"Paper: {doc.metadata.get('Title', 'unknown')}\"\n",
        "        f\"\\n\\nSummary: {doc.metadata.get('Summary', 'unknown')}\"\n",
        "        f\"\\n\\nPage Body: {doc.page_content}\"\n",
        "    )\n",
        "\n",
        "## This printout just confirms that your store has been retrieved\n",
        "pprint(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
        "pprint(f\"Sample Chunk:\")\n",
        "print(format_chunk(docs[len(docs)//2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dib0F-t2N4LJ",
      "metadata": {
        "id": "dib0F-t2N4LJ"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Task 2: [Exercise]** Pull In Your RAG Chain\n",
        "\n",
        "Now that we have our index, we can recreate the RAG agent from the previous notebook!\n",
        "\n",
        "**Key Modifications:**\n",
        "- To keep things simple, feel free to disregard the vectorstore-as-a-memory component. Incorporating it will require some more overhead and will make the exercise a bit more complicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XBi6Y8b8aXd2",
      "metadata": {
        "id": "XBi6Y8b8aXd2",
        "outputId": "a5ff57a2-c857-4cdd-c195-5b9d799f45f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Did you know that there's a document called the \"Archibald Thorburn's Birds\" from the late 1800s that was originally bound with a fish? Sounds strange, right? It turns out that the editor, Archibald Thorburn, was also a talented taxidermy artist, and he had attached a fish to the cover of the book. Isn't that a fascinating piece of document history?"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain.document_transformers import LongContextReorder\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "#####################################################################\n",
        "## TODO: Update as necessary and add new components\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "llm = instruct_llm | StrOutputParser()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "#####################################################################\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name: out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked you a question: {input}\\n\\n\"\n",
        "    \" The following information may be useful for your response: \"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "def output_puller(inputs):\n",
        "    \"\"\"\"Output generator. Useful if your chain returns a dictionary with key 'output'\"\"\"\n",
        "    for token in inputs:\n",
        "        if token.get('output'):\n",
        "            yield token.get('output')\n",
        "\n",
        "#####################################################################\n",
        "## TODO: Pull in your desired RAG Chain. Memory not necessary\n",
        "\n",
        "## Chain 1 Specs: \"Hello World\" -> retrieval_chain\n",
        "## -> {'input': <str>, 'context' : <str>}\n",
        "\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents) ## GIVEN\n",
        "context_getter = RunnableLambda(lambda x: x) ## TODO\n",
        "retrieval_chain = {'input' : (lambda x: x)} | RunnableAssign({'context' : context_getter})\n",
        "\n",
        "## Chain 2 Specs: retrieval_chain -> generator_chain\n",
        "## -> {\"output\" : <str>, ...} -> output_puller\n",
        "\n",
        "generator_chain = RunnableLambda(lambda x: x) ## TODO\n",
        "generator_chain = {'output' : generator_chain} | RunnableLambda(output_puller) ## GIVEN\n",
        "\n",
        "## END TODO\n",
        "#####################################################################\n",
        "\n",
        "rag_chain = retrieval_chain | generator_chain\n",
        "\n",
        "# pprint(rag_chain.invoke(\"Tell me something interesting!\"))\n",
        "for token in rag_chain.stream(\"Tell me something interesting!\"):\n",
        "    print(token, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8",
      "metadata": {
        "id": "2b880971-d3a0-433f-a60b-e8a4edb754c8"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Step 3:** Generating Synthetic Question-Answer Pairs\n",
        "\n",
        "In this section, we can implement the first few part of our evaluation routine:\n",
        "\n",
        "- **Sample the RAG agent document pool to find two document chunks.**\n",
        "- **Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.**\n",
        "- Use the RAG agent to generate its own answer.\n",
        "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
        "\n",
        "The chain should be a simple but powerful process that tests for the following objective:\n",
        "\n",
        "> Does my RAG chain outperform a narrow chatbot with limited document access?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ymzuX-DSNvL6",
      "metadata": {
        "id": "ymzuX-DSNvL6",
        "outputId": "657b9e61-cd68-4756-9176-8f937de913ec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do large language models (LMs) struggle when it comes to understanding and solving certain types of </span>\n",
              "<span style=\"font-weight: bold\">arithmetic word problems, and how can the MRKL system help mitigate these limitations?</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQuestion: How do large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m struggle when it comes to understanding and solving certain types of \u001b[0m\n",
              "\u001b[1marithmetic word problems, and how can the MRKL system help mitigate these limitations?\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: The MRKL system incorporates external knowledge sources and discrete reasoning modules to complement LMs, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">enabling improved performance on arithmetic word problems, even when the problems are phrased differently or </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">require world knowledge to understand.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: The MRKL system incorporates external knowledge sources and discrete reasoning modules to complement LMs, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0menabling improved performance on arithmetic word problems, even when the problems are phrased differently or \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrequire world knowledge to understand.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do retrieval-augmented generation (RAG) models compare to task-specific architectures on </span>\n",
              "<span style=\"font-weight: bold\">knowledge-intensive NLP tasks, and can they overcome the limitations of pre-trained language models in accessing </span>\n",
              "<span style=\"font-weight: bold\">and precisely manipulating knowledge?</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQuestion: How do retrieval-augmented generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m models compare to task-specific architectures on \u001b[0m\n",
              "\u001b[1mknowledge-intensive NLP tasks, and can they overcome the limitations of pre-trained language models in accessing \u001b[0m\n",
              "\u001b[1mand precisely manipulating knowledge?\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the paper on Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, RAG models can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">outperform task-specific retrieve-and-extract architectures and parametric seq2seq models on knowledge-intensive </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">NLP tasks, achieving state-of-the-art results on three open domain QA tasks, and generating more specific, diverse,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and factual language than a state-of-the-art parametric-only seq2seq baseline.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: According to the paper on Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, RAG models can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0moutperform task-specific retrieve-and-extract architectures and parametric seq2seq models on knowledge-intensive \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mNLP tasks, achieving state-of-the-art results on three open domain QA tasks, and generating more specific, diverse,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mand factual language than a state-of-the-art parametric-only seq2seq baseline.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: How do the performance and efficiency of large language models impact the balance between training cost, </span>\n",
              "<span style=\"font-weight: bold\">inference cost, and model capabilities?</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQuestion: How do the performance and efficiency of large language models impact the balance between training cost, \u001b[0m\n",
              "\u001b[1minference cost, and model capabilities?\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Answer: According to the Mistral 7B paper, the field of large language models has so far focused on scaling laws in</span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> dimensions, directly associating model capabilities with training cost. However, the problem is rather </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dimensional, involving model capabilities, training cost, and inference cost, indicating that much remains to be </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">explored to obtain the best performance with the smallest possible model. This suggests that researchers must </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">consider multiple factors when optimizing large language models, rather than just focusing on one or two </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dimensions.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAnswer: According to the Mistral 7B paper, the field of large language models has so far focused on scaling laws in\u001b[0m\n",
              "\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m dimensions, directly associating model capabilities with training cost. However, the problem is rather \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdimensional, involving model capabilities, training cost, and inference cost, indicating that much remains to be \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mexplored to obtain the best performance with the smallest possible model. This suggests that researchers must \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mconsider multiple factors when optimizing large language models, rather than just focusing on one or two \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdimensions.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "num_questions = 3\n",
        "synth_questions = []\n",
        "synth_answers = []\n",
        "\n",
        "simple_prompt = ChatPromptTemplate.from_messages([('system', '{system}'), ('user', 'INPUT: {input}')])\n",
        "\n",
        "for i in range(num_questions):\n",
        "    doc1, doc2 = random.sample(docs, 2)\n",
        "    sys_msg = (\n",
        "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
        "        \" Try to use both documents if possible, and rely more on the document bodies than the summary.\"\n",
        "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
        "        \" DO NOT SAY: \\\"Here is an interesting question pair\\\" or similar. FOLLOW FORMAT!\"\n",
        "    )\n",
        "    usr_msg = (\n",
        "        f\"Document1: {format_chunk(doc1)}\\n\\n\"\n",
        "        f\"Document2: {format_chunk(doc2)}\"\n",
        "    )\n",
        "\n",
        "    # Generate the QA pair using the LLM\n",
        "    qa_pair = (simple_prompt | llm).invoke({'system': sys_msg, 'input': usr_msg})\n",
        "\n",
        "    # Split the output into question and answer\n",
        "    qa_parts = qa_pair.split('\\n\\n')\n",
        "\n",
        "    # Ensure the output has at least two parts (question and answer)\n",
        "    if len(qa_parts) >= 2:\n",
        "        question = qa_parts[0].strip()\n",
        "        answer = qa_parts[1].strip()\n",
        "\n",
        "        # Store the question and answer\n",
        "        synth_questions.append(question)\n",
        "        synth_answers.append(answer)\n",
        "\n",
        "        # Print the results\n",
        "        pprint2(f\"QA Pair {i+1}\")\n",
        "        pprint2(question)\n",
        "        pprint(answer)\n",
        "        print()\n",
        "    else:\n",
        "        # Handle cases where the format is incorrect\n",
        "        print(f\"Warning: Unexpected format for QA Pair {i+1}. Skipping this pair.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5Q-3X4vS98P",
      "metadata": {
        "id": "c5Q-3X4vS98P"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Step 4:** Answer The Synthetic Questions\n",
        "\n",
        "In this section, we can implement the third part of our evaluation routine:\n",
        "\n",
        "- Sample the RAG agent document pool to find two document chunks.\n",
        "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
        "- **Use the RAG agent to generate its own answer.**\n",
        "- Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"\n",
        "\n",
        "The chain should be a simple but powerful process that tests for the following objective:\n",
        "\n",
        "> Does my RAG chain outperform a narrow chatbot with limited document access?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7T3GSwhZPHjF",
      "metadata": {
        "id": "7T3GSwhZPHjF",
        "outputId": "ad1f6997-818e-4491-e5ae-37dd4aba9d97"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "<span style=\"font-weight: bold\">Question: How do large language models (LMs) struggle when it comes to understanding and solving certain types of </span>\n",
              "<span style=\"font-weight: bold\">arithmetic word problems, and how can the MRKL system help mitigate these limitations?</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m1\u001b[0m\n",
              "\u001b[1mQuestion: How do large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m struggle when it comes to understanding and solving certain types of \u001b[0m\n",
              "\u001b[1marithmetic word problems, and how can the MRKL system help mitigate these limitations?\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer:  Large language models (LMs) are incredibly powerful tools, but they're not perfect, especially when it</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">comes to math problems. They often struggle with understanding and solving certain types of arithmetic word </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">problems because they rely heavily on natural language processing (NLP) and may not always be able to efficiently </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">extract the relevant information.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As discussed in the MRKL Systems paper, large language models have limitations, such as:</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* A lack of current and/or proprietary information</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* An inability to reason symbolically when needed</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This can make it difficult for LMs to accurately solve math problems that require reasoning and symbol </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">manipulation.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Now, that's where the MRKL system comes in! The Modular Reasoning, Knowledge, and Language (MRKL) system is </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">designed to address these limitations by combining large language models with:</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* External knowledge sources</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Discrete reasoning modules</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The MRKL system's flexible, neuro-symbolic design allows it to retain the benefits of modern LMs while avoiding </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">their limitations. This means that the MRKL system can handle basic arithmetic reliably and address the challenges </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of cross-domain reasoning.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As demonstrated by AI21 Labs' implementation of the MRKL system, the few-shot setting is shown to have limited </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance in experiments. Instead, a systematic approach is recommended.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">By using the MRKL system, we can expect to see improved performance in solving arithmetic word problems that </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">require reasoning and symbol manipulation. The MRKL system's modular design also makes it easy to extend and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">update, which allows it to better handle new and complex tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In short, the MRKL system is designed to help mitigate the limitations of large language models in solving </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">arithmetic word problems by providing a more robust and flexible architecture that combines the benefits of NLP </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with discrete reasoning and external knowledge.</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer:  Large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m are incredibly powerful tools, but they're not perfect, especially when it\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mcomes to math problems. They often struggle with understanding and solving certain types of arithmetic word \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mproblems because they rely heavily on natural language processing \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mNLP\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and may not always be able to efficiently \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mextract the relevant information.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAs discussed in the MRKL Systems paper, large language models have limitations, such as:\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m* A lack of current and/or proprietary information\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m* An inability to reason symbolically when needed\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThis can make it difficult for LMs to accurately solve math problems that require reasoning and symbol \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmanipulation.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mNow, that's where the MRKL system comes in! The Modular Reasoning, Knowledge, and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system is \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdesigned to address these limitations by combining large language models with:\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m* External knowledge sources\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m* Discrete reasoning modules\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThe MRKL system's flexible, neuro-symbolic design allows it to retain the benefits of modern LMs while avoiding \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtheir limitations. This means that the MRKL system can handle basic arithmetic reliably and address the challenges \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mof cross-domain reasoning.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAs demonstrated by AI21 Labs' implementation of the MRKL system, the few-shot setting is shown to have limited \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mperformance in experiments. Instead, a systematic approach is recommended.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mBy using the MRKL system, we can expect to see improved performance in solving arithmetic word problems that \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrequire reasoning and symbol manipulation. The MRKL system's modular design also makes it easy to extend and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mupdate, which allows it to better handle new and complex tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mIn short, the MRKL system is designed to help mitigate the limitations of large language models in solving \u001b[0m\n",
              "\u001b[1;38;2;118;185;0marithmetic word problems by providing a more robust and flexible architecture that combines the benefits of NLP \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mwith discrete reasoning and external knowledge.\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "<span style=\"font-weight: bold\">Question: How do retrieval-augmented generation (RAG) models compare to task-specific architectures on </span>\n",
              "<span style=\"font-weight: bold\">knowledge-intensive NLP tasks, and can they overcome the limitations of pre-trained language models in accessing </span>\n",
              "<span style=\"font-weight: bold\">and precisely manipulating knowledge?</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m2\u001b[0m\n",
              "\u001b[1mQuestion: How do retrieval-augmented generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m models compare to task-specific architectures on \u001b[0m\n",
              "\u001b[1mknowledge-intensive NLP tasks, and can they overcome the limitations of pre-trained language models in accessing \u001b[0m\n",
              "\u001b[1mand precisely manipulating knowledge?\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Retrieval-augmented generation (RAG) models seem to be a game-changer when it comes to tackling </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge-intensive NLP tasks. By combining pre-trained parametric and non-parametric memories, RAG models can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">access and precisely manipulate knowledge in a way that traditional pre-trained language models can't.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The studies cited in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] and [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] show that RAG models outperform parametric seq2seq models and task-specific </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieve-and-extract architectures on a wide range of knowledge-intensive NLP tasks, including open-domain QA </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks. In fact, they set a new state of the art on three open-domain QA tasks: Natural Questions, WebQuestions, and</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">CuratedTrec.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">One of the key advantages of RAG models is that they can access knowledge directly, without relying on </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parameterized implicit knowledge bases. This means that they can revise and expand their knowledge as needed, which</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">is a major limitation of traditional pre-trained language models.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Furthermore, as shown in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], RAG models can generate more specific, diverse, and factual language than a </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art parametric-only seq2seq baseline. This is especially important for knowledge-intensive tasks, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">where precision and accuracy are paramount.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Another interesting finding is that RAG models can be updated to reflect changes in the world, as demonstrated in </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. This means that their knowledge base can be revised and expanded over time, which is a major advantage over </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">traditional pre-trained language models.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, it seems that RAG models have the potential to overcome the limitations of traditional pre-trained </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models in accessing and precisely manipulating knowledge. By combining the best of both worlds  </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pre-trained parametric and non-parametric memories  RAG models offer a powerful new approach to tackling </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge-intensive NLP tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Karpukhin et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Dense Passage Retrieval for Open-Domain Question Answering.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Lami et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Wang et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Khandelwal et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). RETRO: Retrieval-Based Fact-Checking for Question Answering.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Let me know if you have any further questions or if there's anything else I can help with!</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: Retrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models seem to be a game-changer when it comes to tackling \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge-intensive NLP tasks. By combining pre-trained parametric and non-parametric memories, RAG models can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0maccess and precisely manipulate knowledge in a way that traditional pre-trained language models can't.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThe studies cited in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m and \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m show that RAG models outperform parametric seq2seq models and task-specific \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mretrieve-and-extract architectures on a wide range of knowledge-intensive NLP tasks, including open-domain QA \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtasks. In fact, they set a new state of the art on three open-domain QA tasks: Natural Questions, WebQuestions, and\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mCuratedTrec.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOne of the key advantages of RAG models is that they can access knowledge directly, without relying on \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mparameterized implicit knowledge bases. This means that they can revise and expand their knowledge as needed, which\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mis a major limitation of traditional pre-trained language models.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mFurthermore, as shown in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, RAG models can generate more specific, diverse, and factual language than a \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mstate-of-the-art parametric-only seq2seq baseline. This is especially important for knowledge-intensive tasks, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mwhere precision and accuracy are paramount.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAnother interesting finding is that RAG models can be updated to reflect changes in the world, as demonstrated in \u001b[0m\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. This means that their knowledge base can be revised and expanded over time, which is a major advantage over \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtraditional pre-trained language models.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOverall, it seems that RAG models have the potential to overcome the limitations of traditional pre-trained \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mlanguage models in accessing and precisely manipulating knowledge. By combining the best of both worlds  \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mpre-trained parametric and non-parametric memories  RAG models offer a powerful new approach to tackling \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge-intensive NLP tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Karpukhin et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Dense Passage Retrieval for Open-Domain Question Answering.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Lami et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Wang et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Khandelwal et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. RETRO: Retrieval-Based Fact-Checking for Question Answering.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mLet me know if you have any further questions or if there's anything else I can help with!\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">QA Pair </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "<span style=\"font-weight: bold\">Question: How do the performance and efficiency of large language models impact the balance between training cost, </span>\n",
              "<span style=\"font-weight: bold\">inference cost, and model capabilities?</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mQA Pair \u001b[0m\u001b[1;36m3\u001b[0m\n",
              "\u001b[1mQuestion: How do the performance and efficiency of large language models impact the balance between training cost, \u001b[0m\n",
              "\u001b[1minference cost, and model capabilities?\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: As you know, large language models have been a game-changer in the field of Natural Language Processing</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(NLP), but they come with a price. The performance and efficiency of these models have a significant impact on the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">balance between training cost, inference cost, and model capabilities.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Let's start with the training cost. Currently, training large language models requires massive computational </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">resources and can be extremely costly. This is because the models need to be trained on a large dataset, which can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">be quite time-consuming and expensive. For instance, the studies cited in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] trained their models on a </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">massive multi-task setting, which required a significant amount of computational resources and cost.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, inference cost refers to the cost of using the trained model to generate outputs. This cost can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">be significant, especially when dealing with large models. For example, [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] suggests that the problem is rather </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-dimensional (model capabilities, training cost, inference cost), and much remains to be explored to obtain the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">best performance with the smallest possible model.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Now, let's talk about model capabilities. Large language models have been shown to be highly versatile and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">effective in many NLP tasks, but their performance can deteriorate significantly when dealing with tasks that are </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">far removed from their training data. This is because the models tend to overfit to the training data, which can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lead to catastrophic forgetting when new tasks are added.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To mitigate these issues, researchers have proposed various strategies, such as fine-tuning the model on specific </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks or using transfer learning. However, these approaches come with their own set of challenges. For instance, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">fine-tuning a large language model on a new task can be computationally expensive and may not always yield the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">desired results.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Our solution, the Modular Reasoning, Knowledge and Language (MRKL) system, addresses these challenges by providing </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a flexible architecture that combines the benefits of large language models without suffering from their drawbacks.</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">By leveraging external knowledge sources and discrete reasoning, MRKL enables more efficient inference and reduces </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the need for massive multi-task training.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In summary, the performance and efficiency of large language models impact the balance between training cost, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inference cost, and model capabilities in complex ways. By understanding these trade-offs and developing more </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficient and flexible architectures, such as MRKL, we can unlock the full potential of large language models and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">push the boundaries of NLP even further.</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: As you know, large language models have been a game-changer in the field of Natural Language Processing\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mNLP\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, but they come with a price. The performance and efficiency of these models have a significant impact on the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbalance between training cost, inference cost, and model capabilities.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mLet's start with the training cost. Currently, training large language models requires massive computational \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mresources and can be extremely costly. This is because the models need to be trained on a large dataset, which can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbe quite time-consuming and expensive. For instance, the studies cited in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m trained their models on a \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmassive multi-task setting, which required a significant amount of computational resources and cost.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOn the other hand, inference cost refers to the cost of using the trained model to generate outputs. This cost can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbe significant, especially when dealing with large models. For example, \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m suggests that the problem is rather \u001b[0m\n",
              "\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m-dimensional \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mmodel capabilities, training cost, inference cost\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, and much remains to be explored to obtain the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbest performance with the smallest possible model.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mNow, let's talk about model capabilities. Large language models have been shown to be highly versatile and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0meffective in many NLP tasks, but their performance can deteriorate significantly when dealing with tasks that are \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfar removed from their training data. This is because the models tend to overfit to the training data, which can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mlead to catastrophic forgetting when new tasks are added.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mTo mitigate these issues, researchers have proposed various strategies, such as fine-tuning the model on specific \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtasks or using transfer learning. However, these approaches come with their own set of challenges. For instance, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfine-tuning a large language model on a new task can be computationally expensive and may not always yield the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdesired results.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOur solution, the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, addresses these challenges by providing \u001b[0m\n",
              "\u001b[1;38;2;118;185;0ma flexible architecture that combines the benefits of large language models without suffering from their drawbacks.\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mBy leveraging external knowledge sources and discrete reasoning, MRKL enables more efficient inference and reduces \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthe need for massive multi-task training.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mIn summary, the performance and efficiency of large language models impact the balance between training cost, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minference cost, and model capabilities in complex ways. By understanding these trade-offs and developing more \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mefficient and flexible architectures, such as MRKL, we can unlock the full potential of large language models and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mpush the boundaries of NLP even further.\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## TODO: Generate some synthetic answers to the questions above.\n",
        "##   Try to use the same syntax as the cell above\n",
        "rag_answers = []\n",
        "for i, q in enumerate(synth_questions):\n",
        "    ## TODO: Compute the RAG Answer\n",
        "    rag_answer = \"\"\n",
        "    rag_answers += [rag_answer]\n",
        "    pprint2(f\"QA Pair {i+1}\", q, \"\", sep=\"\\n\")\n",
        "    pprint(f\"RAG Answer: {rag_answer}\", \"\", sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ho5cnN_Xt_yr",
      "metadata": {
        "id": "Ho5cnN_Xt_yr"
      },
      "source": [
        "<br>\n",
        "\n",
        "### **Step 5:** Implement A Human Preference Metric\n",
        "\n",
        "In this section, we can implement the fourth part of our evaluation routine:\n",
        "\n",
        "- Sample the RAG agent document pool to find two document chunks.\n",
        "- Use those two document chunks to generate a synthetic \"baseline\" question-answer pair.\n",
        "- Use the RAG agent to generate its own answer.\n",
        "- **Use a judge LLM to compare the two responses while grounding the synthetic generation as \"ground-truth correct.\"**\n",
        "\n",
        "The chain should be a simple but powerful process that tests for the following objective:\n",
        "\n",
        "> Does my RAG chain outperform a narrow chatbot with limited document access?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sf6f2oFLuPtu",
      "metadata": {
        "id": "sf6f2oFLuPtu",
        "outputId": "c2bbc1f0-a3d9-485b-c800-75da53cebe66"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: How do large language models (LMs) struggle when it comes to understanding and solving certain </span>\n",
              "<span style=\"font-weight: bold\">types of arithmetic word problems, and how can the MRKL system help mitigate these limitations?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m1\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: How do large language models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m struggle when it comes to understanding and solving certain \u001b[0m\n",
              "\u001b[1mtypes of arithmetic word problems, and how can the MRKL system help mitigate these limitations?\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: The MRKL system incorporates external knowledge sources and discrete reasoning modules to </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">complement LMs, enabling improved performance on arithmetic word problems, even when the problems are phrased </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">differently or require world knowledge to understand.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: The MRKL system incorporates external knowledge sources and discrete reasoning modules to \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mcomplement LMs, enabling improved performance on arithmetic word problems, even when the problems are phrased \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdifferently or require world knowledge to understand.\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer:  Large language models (LMs) are incredibly powerful tools, but they're not perfect, especially when it</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">comes to math problems. They often struggle with understanding and solving certain types of arithmetic word </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">problems because they rely heavily on natural language processing (NLP) and may not always be able to efficiently </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">extract the relevant information.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As discussed in the MRKL Systems paper, large language models have limitations, such as:</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* A lack of current and/or proprietary information</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* An inability to reason symbolically when needed</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This can make it difficult for LMs to accurately solve math problems that require reasoning and symbol </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">manipulation.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Now, that's where the MRKL system comes in! The Modular Reasoning, Knowledge, and Language (MRKL) system is </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">designed to address these limitations by combining large language models with:</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* External knowledge sources</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">* Discrete reasoning modules</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The MRKL system's flexible, neuro-symbolic design allows it to retain the benefits of modern LMs while avoiding </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">their limitations. This means that the MRKL system can handle basic arithmetic reliably and address the challenges </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of cross-domain reasoning.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As demonstrated by AI21 Labs' implementation of the MRKL system, the few-shot setting is shown to have limited </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">performance in experiments. Instead, a systematic approach is recommended.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">By using the MRKL system, we can expect to see improved performance in solving arithmetic word problems that </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">require reasoning and symbol manipulation. The MRKL system's modular design also makes it easy to extend and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">update, which allows it to better handle new and complex tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In short, the MRKL system is designed to help mitigate the limitations of large language models in solving </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">arithmetic word problems by providing a more robust and flexible architecture that combines the benefits of NLP </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with discrete reasoning and external knowledge.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer:  Large language models \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mLMs\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m are incredibly powerful tools, but they're not perfect, especially when it\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mcomes to math problems. They often struggle with understanding and solving certain types of arithmetic word \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mproblems because they rely heavily on natural language processing \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mNLP\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m and may not always be able to efficiently \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mextract the relevant information.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAs discussed in the MRKL Systems paper, large language models have limitations, such as:\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m* A lack of current and/or proprietary information\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m* An inability to reason symbolically when needed\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThis can make it difficult for LMs to accurately solve math problems that require reasoning and symbol \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmanipulation.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mNow, that's where the MRKL system comes in! The Modular Reasoning, Knowledge, and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system is \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdesigned to address these limitations by combining large language models with:\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m* External knowledge sources\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m* Discrete reasoning modules\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThe MRKL system's flexible, neuro-symbolic design allows it to retain the benefits of modern LMs while avoiding \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtheir limitations. This means that the MRKL system can handle basic arithmetic reliably and address the challenges \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mof cross-domain reasoning.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAs demonstrated by AI21 Labs' implementation of the MRKL system, the few-shot setting is shown to have limited \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mperformance in experiments. Instead, a systematic approach is recommended.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mBy using the MRKL system, we can expect to see improved performance in solving arithmetic word problems that \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrequire reasoning and symbol manipulation. The MRKL system's modular design also makes it easy to extend and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mupdate, which allows it to better handle new and complex tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mIn short, the MRKL system is designed to help mitigate the limitations of large language models in solving \u001b[0m\n",
              "\u001b[1;38;2;118;185;0marithmetic word problems by providing a more robust and flexible architecture that combines the benefits of NLP \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mwith discrete reasoning and external knowledge.\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> Justification</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">The second answer is longer and more detailed than the first, but its content and structure are very similar. </span>\n",
              "<span style=\"font-weight: bold\">However, the second answer avoids repetition and provides more nuanced information about the limitations of large </span>\n",
              "<span style=\"font-weight: bold\">language models (LMs) and the advantages of the MRKL system. The second answer also explains the concept of modular</span>\n",
              "<span style=\"font-weight: bold\">reasoning and its relevance to the MRKL system, which adds value to the original question.</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">The second answer does not introduce any inconsistencies and answers the question more thoroughly than the first. </span>\n",
              "<span style=\"font-weight: bold\">The text layout and explanations are clear, making it easier to understand the MRKL system's concept and benefits. </span>\n",
              "<span style=\"font-weight: bold\">Overall, the second answer is better than the first and provides a more comprehensive response.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m Justification\u001b[0m\n",
              "\n",
              "\u001b[1mThe second answer is longer and more detailed than the first, but its content and structure are very similar. \u001b[0m\n",
              "\u001b[1mHowever, the second answer avoids repetition and provides more nuanced information about the limitations of large \u001b[0m\n",
              "\u001b[1mlanguage models \u001b[0m\u001b[1m(\u001b[0m\u001b[1mLMs\u001b[0m\u001b[1m)\u001b[0m\u001b[1m and the advantages of the MRKL system. The second answer also explains the concept of modular\u001b[0m\n",
              "\u001b[1mreasoning and its relevance to the MRKL system, which adds value to the original question.\u001b[0m\n",
              "\n",
              "\u001b[1mThe second answer does not introduce any inconsistencies and answers the question more thoroughly than the first. \u001b[0m\n",
              "\u001b[1mThe text layout and explanations are clear, making it easier to understand the MRKL system's concept and benefits. \u001b[0m\n",
              "\u001b[1mOverall, the second answer is better than the first and provides a more comprehensive response.\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: How do retrieval-augmented generation (RAG) models compare to task-specific architectures on </span>\n",
              "<span style=\"font-weight: bold\">knowledge-intensive NLP tasks, and can they overcome the limitations of pre-trained language models in accessing </span>\n",
              "<span style=\"font-weight: bold\">and precisely manipulating knowledge?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m2\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: How do retrieval-augmented generation \u001b[0m\u001b[1m(\u001b[0m\u001b[1mRAG\u001b[0m\u001b[1m)\u001b[0m\u001b[1m models compare to task-specific architectures on \u001b[0m\n",
              "\u001b[1mknowledge-intensive NLP tasks, and can they overcome the limitations of pre-trained language models in accessing \u001b[0m\n",
              "\u001b[1mand precisely manipulating knowledge?\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the paper on Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG models can outperform task-specific retrieve-and-extract architectures and parametric seq2seq models on </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge-intensive NLP tasks, achieving state-of-the-art results on three open domain QA tasks, and generating </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">more specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the paper on Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mRAG models can outperform task-specific retrieve-and-extract architectures and parametric seq2seq models on \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge-intensive NLP tasks, achieving state-of-the-art results on three open domain QA tasks, and generating \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmore specific, diverse, and factual language than a state-of-the-art parametric-only seq2seq baseline.\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: Retrieval-augmented generation (RAG) models seem to be a game-changer when it comes to tackling </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge-intensive NLP tasks. By combining pre-trained parametric and non-parametric memories, RAG models can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">access and precisely manipulate knowledge in a way that traditional pre-trained language models can't.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The studies cited in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] and [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] show that RAG models outperform parametric seq2seq models and task-specific </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">retrieve-and-extract architectures on a wide range of knowledge-intensive NLP tasks, including open-domain QA </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks. In fact, they set a new state of the art on three open-domain QA tasks: Natural Questions, WebQuestions, and</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">CuratedTrec.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">One of the key advantages of RAG models is that they can access knowledge directly, without relying on </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">parameterized implicit knowledge bases. This means that they can revise and expand their knowledge as needed, which</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">is a major limitation of traditional pre-trained language models.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Furthermore, as shown in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], RAG models can generate more specific, diverse, and factual language than a </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">state-of-the-art parametric-only seq2seq baseline. This is especially important for knowledge-intensive tasks, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">where precision and accuracy are paramount.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Another interesting finding is that RAG models can be updated to reflect changes in the world, as demonstrated in </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. This means that their knowledge base can be revised and expanded over time, which is a major advantage over </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">traditional pre-trained language models.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Overall, it seems that RAG models have the potential to overcome the limitations of traditional pre-trained </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language models in accessing and precisely manipulating knowledge. By combining the best of both worlds  </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">pre-trained parametric and non-parametric memories  RAG models offer a powerful new approach to tackling </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">knowledge-intensive NLP tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">References:</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Karpukhin et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Dense Passage Retrieval for Open-Domain Question Answering.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Lami et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Wang et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] Khandelwal et al. (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2020</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">). RETRO: Retrieval-Based Fact-Checking for Question Answering.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Let me know if you have any further questions or if there's anything else I can help with!</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: Retrieval-augmented generation \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mRAG\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m models seem to be a game-changer when it comes to tackling \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge-intensive NLP tasks. By combining pre-trained parametric and non-parametric memories, RAG models can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0maccess and precisely manipulate knowledge in a way that traditional pre-trained language models can't.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mThe studies cited in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m and \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m show that RAG models outperform parametric seq2seq models and task-specific \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mretrieve-and-extract architectures on a wide range of knowledge-intensive NLP tasks, including open-domain QA \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtasks. In fact, they set a new state of the art on three open-domain QA tasks: Natural Questions, WebQuestions, and\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mCuratedTrec.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOne of the key advantages of RAG models is that they can access knowledge directly, without relying on \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mparameterized implicit knowledge bases. This means that they can revise and expand their knowledge as needed, which\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mis a major limitation of traditional pre-trained language models.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mFurthermore, as shown in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, RAG models can generate more specific, diverse, and factual language than a \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mstate-of-the-art parametric-only seq2seq baseline. This is especially important for knowledge-intensive tasks, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mwhere precision and accuracy are paramount.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mAnother interesting finding is that RAG models can be updated to reflect changes in the world, as demonstrated in \u001b[0m\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. This means that their knowledge base can be revised and expanded over time, which is a major advantage over \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtraditional pre-trained language models.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOverall, it seems that RAG models have the potential to overcome the limitations of traditional pre-trained \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mlanguage models in accessing and precisely manipulating knowledge. By combining the best of both worlds  \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mpre-trained parametric and non-parametric memories  RAG models offer a powerful new approach to tackling \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mknowledge-intensive NLP tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mReferences:\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Karpukhin et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Dense Passage Retrieval for Open-Domain Question Answering.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Lami et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Wang et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m Khandelwal et al. \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2020\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. RETRO: Retrieval-Based Fact-Checking for Question Answering.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mLet me know if you have any further questions or if there's anything else I can help with!\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation:  </span>\n",
              "\n",
              "<span style=\"font-weight: bold\">[Score] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\"> Justification </span>\n",
              "\n",
              "<span style=\"font-weight: bold\">The ground truth answer (Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">) is reproduced verbatim in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">. Although Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> provides further </span>\n",
              "<span style=\"font-weight: bold\">elaboration and supporting evidence for the claims made by Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">, it does not introduce any new information that</span>\n",
              "<span style=\"font-weight: bold\">contradicts the original answer. However, due to the fact that the majority of the content in Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> is a </span>\n",
              "<span style=\"font-weight: bold\">paraphrased version of Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">, I would argue that Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\"> only contributes minor value to the conversation while</span>\n",
              "<span style=\"font-weight: bold\">Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> explicitly addresses the question. Thus, Answer </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\"> remains the superior answer.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mSynth Evaluation:  \u001b[0m\n",
              "\n",
              "\u001b[1m[\u001b[0m\u001b[1mScore\u001b[0m\u001b[1m]\u001b[0m\u001b[1m \u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1m Justification \u001b[0m\n",
              "\n",
              "\u001b[1mThe ground truth answer \u001b[0m\u001b[1m(\u001b[0m\u001b[1mAnswer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m is reproduced verbatim in Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m. Although Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m provides further \u001b[0m\n",
              "\u001b[1melaboration and supporting evidence for the claims made by Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m, it does not introduce any new information that\u001b[0m\n",
              "\u001b[1mcontradicts the original answer. However, due to the fact that the majority of the content in Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m is a \u001b[0m\n",
              "\u001b[1mparaphrased version of Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m, I would argue that Answer \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m only contributes minor value to the conversation while\u001b[0m\n",
              "\u001b[1mAnswer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m explicitly addresses the question. Thus, Answer \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m remains the superior answer.\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Set </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "\n",
              "<span style=\"font-weight: bold\">Question: Question: How do the performance and efficiency of large language models impact the balance between </span>\n",
              "<span style=\"font-weight: bold\">training cost, inference cost, and model capabilities?</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mSet \u001b[0m\u001b[1;36m3\u001b[0m\n",
              "\n",
              "\u001b[1mQuestion: Question: How do the performance and efficiency of large language models impact the balance between \u001b[0m\n",
              "\u001b[1mtraining cost, inference cost, and model capabilities?\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Synth Answer: Answer: According to the Mistral 7B paper, the field of large language models has so far focused on </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">scaling laws in </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> dimensions, directly associating model capabilities with training cost. However, the problem is </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">rather </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> dimensional, involving model capabilities, training cost, and inference cost, indicating that much remains</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to be explored to obtain the best performance with the smallest possible model. This suggests that researchers must</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">consider multiple factors when optimizing large language models, rather than just focusing on one or two </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dimensions.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mSynth Answer: Answer: According to the Mistral 7B paper, the field of large language models has so far focused on \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mscaling laws in \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m dimensions, directly associating model capabilities with training cost. However, the problem is \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mrather \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m dimensional, involving model capabilities, training cost, and inference cost, indicating that much remains\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mto be explored to obtain the best performance with the smallest possible model. This suggests that researchers must\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mconsider multiple factors when optimizing large language models, rather than just focusing on one or two \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdimensions.\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">RAG Answer: As you know, large language models have been a game-changer in the field of Natural Language Processing</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(NLP), but they come with a price. The performance and efficiency of these models have a significant impact on the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">balance between training cost, inference cost, and model capabilities.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Let's start with the training cost. Currently, training large language models requires massive computational </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">resources and can be extremely costly. This is because the models need to be trained on a large dataset, which can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">be quite time-consuming and expensive. For instance, the studies cited in [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] trained their models on a </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">massive multi-task setting, which required a significant amount of computational resources and cost.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the other hand, inference cost refers to the cost of using the trained model to generate outputs. This cost can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">be significant, especially when dealing with large models. For example, [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] suggests that the problem is rather </span>\n",
              "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-dimensional (model capabilities, training cost, inference cost), and much remains to be explored to obtain the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">best performance with the smallest possible model.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Now, let's talk about model capabilities. Large language models have been shown to be highly versatile and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">effective in many NLP tasks, but their performance can deteriorate significantly when dealing with tasks that are </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">far removed from their training data. This is because the models tend to overfit to the training data, which can </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lead to catastrophic forgetting when new tasks are added.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To mitigate these issues, researchers have proposed various strategies, such as fine-tuning the model on specific </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tasks or using transfer learning. However, these approaches come with their own set of challenges. For instance, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">fine-tuning a large language model on a new task can be computationally expensive and may not always yield the </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">desired results.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Our solution, the Modular Reasoning, Knowledge and Language (MRKL) system, addresses these challenges by providing </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a flexible architecture that combines the benefits of large language models without suffering from their drawbacks.</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">By leveraging external knowledge sources and discrete reasoning, MRKL enables more efficient inference and reduces </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the need for massive multi-task training.</span>\n",
              "\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In summary, the performance and efficiency of large language models impact the balance between training cost, </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inference cost, and model capabilities in complex ways. By understanding these trade-offs and developing more </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">efficient and flexible architectures, such as MRKL, we can unlock the full potential of large language models and </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">push the boundaries of NLP even further.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRAG Answer: As you know, large language models have been a game-changer in the field of Natural Language Processing\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mNLP\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, but they come with a price. The performance and efficiency of these models have a significant impact on the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbalance between training cost, inference cost, and model capabilities.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mLet's start with the training cost. Currently, training large language models requires massive computational \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mresources and can be extremely costly. This is because the models need to be trained on a large dataset, which can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbe quite time-consuming and expensive. For instance, the studies cited in \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m trained their models on a \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mmassive multi-task setting, which required a significant amount of computational resources and cost.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOn the other hand, inference cost refers to the cost of using the trained model to generate outputs. This cost can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbe significant, especially when dealing with large models. For example, \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m suggests that the problem is rather \u001b[0m\n",
              "\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m-dimensional \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mmodel capabilities, training cost, inference cost\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, and much remains to be explored to obtain the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mbest performance with the smallest possible model.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mNow, let's talk about model capabilities. Large language models have been shown to be highly versatile and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0meffective in many NLP tasks, but their performance can deteriorate significantly when dealing with tasks that are \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfar removed from their training data. This is because the models tend to overfit to the training data, which can \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mlead to catastrophic forgetting when new tasks are added.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mTo mitigate these issues, researchers have proposed various strategies, such as fine-tuning the model on specific \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mtasks or using transfer learning. However, these approaches come with their own set of challenges. For instance, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mfine-tuning a large language model on a new task can be computationally expensive and may not always yield the \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mdesired results.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mOur solution, the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mMRKL\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m system, addresses these challenges by providing \u001b[0m\n",
              "\u001b[1;38;2;118;185;0ma flexible architecture that combines the benefits of large language models without suffering from their drawbacks.\u001b[0m\n",
              "\u001b[1;38;2;118;185;0mBy leveraging external knowledge sources and discrete reasoning, MRKL enables more efficient inference and reduces \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mthe need for massive multi-task training.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;118;185;0mIn summary, the performance and efficiency of large language models impact the balance between training cost, \u001b[0m\n",
              "\u001b[1;38;2;118;185;0minference cost, and model capabilities in complex ways. By understanding these trade-offs and developing more \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mefficient and flexible architectures, such as MRKL, we can unlock the full potential of large language models and \u001b[0m\n",
              "\u001b[1;38;2;118;185;0mpush the boundaries of NLP even further.\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Synth Evaluation: [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">] Justification: </span>\n",
              "<span style=\"font-weight: bold\">The second answer lies in a different direction than the first answer by providing extensive details beyond the </span>\n",
              "<span style=\"font-weight: bold\">question asked and delving into the topic of developing more efficient and flexible architectures such as MRKL to </span>\n",
              "<span style=\"font-weight: bold\">mitigate the challenges associated with large language models. It mentions multiple references whereas the first </span>\n",
              "<span style=\"font-weight: bold\">answer is short, sweet, and to the point, accurately pinpointing the complexity of the problem and leaving it open </span>\n",
              "<span style=\"font-weight: bold\">for exploration.</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mSynth Evaluation: \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m Justification: \u001b[0m\n",
              "\u001b[1mThe second answer lies in a different direction than the first answer by providing extensive details beyond the \u001b[0m\n",
              "\u001b[1mquestion asked and delving into the topic of developing more efficient and flexible architectures such as MRKL to \u001b[0m\n",
              "\u001b[1mmitigate the challenges associated with large language models. It mentions multiple references whereas the first \u001b[0m\n",
              "\u001b[1manswer is short, sweet, and to the point, accurately pinpointing the complexity of the problem and leaving it open \u001b[0m\n",
              "\u001b[1mfor exploration.\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## TODO: Adapt this prompt for whichever LLM you're actually interested in using.\n",
        "## If it's llama, maybe system message would be good?\n",
        "eval_prompt = ChatPromptTemplate.from_template(\"\"\"INSTRUCTION:\n",
        "Evaluate the following Question-Answer pair for human preference and consistency.\n",
        "Assume the first answer is a ground truth answer and has to be correct.\n",
        "Assume the second answer may or may not be true.\n",
        "[1] The second answer lies, does not answer the question, or is inferior to the first answer.\n",
        "[2] The second answer is better than the first and does not introduce any inconsistencies.\n",
        "\n",
        "Output Format:\n",
        "[Score] Justification\n",
        "\n",
        "{qa_trio}\n",
        "\n",
        "EVALUATION:\n",
        "\"\"\")\n",
        "\n",
        "pref_score = []\n",
        "\n",
        "trio_gen = zip(synth_questions, synth_answers, rag_answers)\n",
        "for i, (q, a_synth, a_rag) in enumerate(trio_gen):\n",
        "    pprint2(f\"Set {i+1}\\n\\nQuestion: {q}\\n\\n\")\n",
        "\n",
        "    qa_trio = f\"Question: {q}\\n\\nAnswer 1 (Ground Truth): {a_synth}\\n\\n Answer 2 (New Answer): {a_rag}\"\n",
        "    pref_score += [(eval_prompt | llm).invoke({'qa_trio': qa_trio})]\n",
        "    pprint(f\"Synth Answer: {a_synth}\\n\\n\")\n",
        "    pprint(f\"RAG Answer: {a_rag}\\n\\n\")\n",
        "    pprint2(f\"Synth Evaluation: {pref_score[-1]}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f",
      "metadata": {
        "id": "e6595662-9f49-44eb-9868-2a3fdb1fb60f"
      },
      "source": [
        "<br>\n",
        "\n",
        "**Congratulations! We now have an LLM system that reasons about our pipeline and tries to evaluate it!** Now that we have some judge results, we can simply aggregate the results and see how often our formulation was according to an LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3L_q6fMH3i6_",
      "metadata": {
        "id": "3L_q6fMH3i6_",
        "outputId": "3e8ebcc4-321b-4324-92fb-d76a5f6f107d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preference Score: 0.0\n"
          ]
        }
      ],
      "source": [
        "pref_score = sum((\"[2]\" in score) for score in pref_score) / len(pref_score)\n",
        "print(f\"Preference Score: {pref_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf80bf04-118d-44a2-a740-361a756a1d5f",
      "metadata": {
        "id": "cf80bf04-118d-44a2-a740-361a756a1d5f"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 4:** Advanced Formulations\n",
        "\n",
        "The exercise above was meant to prepare you for the final assessment of the course and showcased a simple but effective evaluator chain. The objective and implementation details were provided for you, and the logic for using it probably makes sense now that you've seen it in action.\n",
        "\n",
        "With that being said, this metric was merely a product of us specifying:\n",
        "- **What kind of behavior is important for our pipeline to have?**\n",
        "- **What do we need to do in order to exhibit and evaluate this behavior?**\n",
        "\n",
        "From these two questions, we could have come up with plenty of other evaluation metrics that could have assessed different attributes, incorporated different evaluator chain techniques, and even required different pipeline organization strategies. Though far from an exhaustive list, some common formulations you will likely come across may include:\n",
        "\n",
        "- **Style Evaluation:** Some evaluation formulations can be as simple as \"let me ask some questions and see if the output feels desirable.\" This might be used to see whether a chatbot \"acts like it's supposed to\" based on a description provided to a judge LLM. We're using quotations since this kind of assessment can reasonably be achieved with nothing but prompt engineering and a while loop.\n",
        "\n",
        "- **Ground-Truth Evaluation:** In our chain, we used synthetic generation to create some random questions and answers using a sampling strategy, but in reality you may actually have some representative questions and answers that you need your chatbot to consistently get right! In this case, a modification of the exercise chain above should be implemented and closely monitored as you develop your pipelines.\n",
        "\n",
        "- **Retrieval/Augmentation Evaluation:** This course made many assumptions about what kinds of preprocessing and prompting steps would be good for your pipelines, and much of this was determined by experimentation. Factors such as document preprocessing, chunking strategies, model selection, and prompt specification all played important roles, so creating metrics to validate these decisions may be of interest. This kind of metric might require your pipeline to output your context chunks or may even rely solely on embedding similarity comparisons, so keep this in mind when trying to implement a chain that works with multiple evaluation strategies. Consider the [**RagasEvaluatorChain**](https://docs.ragas.io/en/stable/howtos/integrations/langchain.html) abstraction as a decent starting point for making an custom generalizable evaluation routine.\n",
        "\n",
        "- **Trajectory Evaluation:** Using more advanced agent formulations, you can implement multiple-query strategies that assume the presence of conversational memory. With this, you can implement an evaluation agent which can:\n",
        "    - Ask a series of questions in order to evaluate how well the agent is able to adapt and cater to the scenario. This kind of system generally considers a series of correspondence and aims to tease out and evaluate a \"trajectory\" of how the agent navigated the conversation. The [**LangChain Trajectory Evaluators documentation**](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/trajectory/) is a good starting point.\n",
        "    - Alternatively, you could also implement an evaluation agent that tries to achieve objectives by interacting with the chatbot. Such an agent can output whether they were able to navigate to their solution in a natural manner, and can even be used to generate a report about the percieved performance. The [**LangChain Agents documentation**](https://python.langchain.com/v0.1/docs/modules/agents/) is a good starting point!\n",
        "\n",
        "<br>\n",
        "\n",
        "At the end of the day, just make sure to use the tools you have at your disposal appropriately. By this point in the course, you should already be well-acquainted with the LLM core value propositions: **They're powerful, scalable, predictable, controllable, and orchestratable... but will act unpredictably when you just expect them to work by default.** Assess your needs, formulate and validate your pipelines, give enough information, and add as much control as you can to make your system work consistently, efficiently, and effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61faee2c-e534-4c89-91ae-45c37835dba5",
      "metadata": {
        "id": "61faee2c-e534-4c89-91ae-45c37835dba5"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## **Part 5: [Assessment]** Evaluating For Credit\n",
        "\n",
        "Welcome to the last exercise of the course! Hopefully you've enjoyed the material and are ready to actually get credit for these notebooks! For this part:\n",
        "\n",
        "- **Make sure you're in the course environment**\n",
        "- **Make sure `docstore_index/` has been uploaded to the course environment...**\n",
        "    - **...and contains [at least one Arxiv paper](https://arxiv.org/search/advanced) which has been updated recently.**\n",
        "- **Make sure you don't have some old session of [`09_langserve.ipynb`](09_langserve.ipynb) already occupying the port. Your assessment requires you to implement the new `/retriever` and `/generator` endpoints!!**\n",
        "\n",
        "**Objective:** On launch, [**`frontend/frontend_block.py`**](frontend/frontend_block.py) had several lines of code which trigger the course pass condition. Your objective is to invoke that series of commands by using your pipeline to pass the **Evaluation** check! Recall [`09_langserve.ipynb`](09_langserve.ipynb) and use it as a starting example! As a recommendation, consider duplicating it so that you can keep the original as an authoritative reference.\n",
        "\n",
        "**Once Finished:** While your course environment is still open, please navigate back to your course environment launcher area and click the **\"Assess Task\"** button! After that, you're all done!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e300ed-951c-4006-ac54-cbbd41251707",
      "metadata": {
        "id": "48e300ed-951c-4006-ac54-cbbd41251707",
        "outputId": "2f045bbf-73d7-491e-dc7c-2cd7452405d8"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "var url = 'http://'+window.location.host+':8090';\n",
              "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%js\n",
        "var url = 'http://'+window.location.host+':8090';\n",
        "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'><h1>< Link To Gradio Frontend ></h1></a>';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ad05ca-ae4b-4eb7-9e73-8a345b5f553a",
      "metadata": {
        "id": "34ad05ca-ae4b-4eb7-9e73-8a345b5f553a",
        "outputId": "5637b615-54ed-45d5-dcae-64d2bb4f124b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm delighted to introduce you to the Llama model!\n",
            "\n",
            "Llama stands for \"Large Language Model Meta AI.\" It's an artificial intelligence model developed by Meta AI, a leading AI research organization. The Llama model is a type of transformer-based language model, designed to process and generate human-like language.\n",
            "\n",
            "**Key features of the Llama model:**\n",
            "\n",
            "1. **Large-scale training**: The Llama model was trained on a massive dataset of text from various sources, including books, articles, and websites. This large-scale training enables it to understand and generate text on a wide range of topics.\n",
            "2. **Transformers architecture**: The Llama model uses a transformer architecture, which is a type of neural network that's well-suited for natural language processing tasks. This architecture allows the model to capture long-range dependencies in text and generate coherent text.\n",
            "3. **Multitask learning**: The Llama model is trained on multiple tasks, including language translation, text generation, and question-answering. This multitask learning approach helps the model develop a more comprehensive understanding of language.\n",
            "4. **Self-supervised learning**: The Llama model is trained using self-supervised learning, which means that the model learns from itself by predicting the next word in a sequence of text given the context of the previous words.\n",
            "\n",
            "**Applications of the Llama model:**\n",
            "\n",
            "1. **Conversational AI**: The Llama model is designed to be used in conversational AI applications, such as chatbots and voice assistants. It can engage in natural-sounding conversations and answer questions on a wide range of topics.\n",
            "2. **Text summarization**: The Llama model can summarize long pieces of text into concise and coherent summaries.\n",
            "3. **Language translation**: The Llama model can translate text from one language to another, including popular languages such as Spanish, French, and Chinese.\n",
            "4. **Content creation**: The Llama model can generate essays, articles, and other types of content on specific topics.\n",
            "\n",
            "**Hardware requirements:**\n",
            "\n",
            "To run the Llama model, you'll need a computer with a significant amount of processing power and memory. The exact requirements depend on the specific use case, but as a general rule of thumb:\n",
            "\n",
            "* For small-scale applications, you may need a computer with a CPU of at least 4 cores and 16 GB of RAM.\n",
            "* For larger-scale applications, you may need a computer with a GPU (Graphics Processing Unit) or a cloud-based service that provides ample compute resources.\n",
            "\n",
            "**Limitations and potential applications:**\n",
            "\n",
            "While the Llama model is an impressive achievement in natural language processing, it has some limitations:\n",
            "\n",
            "1. **Common-sense reasoning**: The Llama model may not always be able to apply common-sense reasoning to understand the nuances of language.\n",
            "2. **Sarcasm and idioms**: The Llama model may struggle to understand sarcasm, idioms, and other forms of figurative language.\n",
            "3. **Real-world applications**: The Llama model's ability to generate text on a wide range of topics may not translate directly to real-world applications, where context and specific knowledge are crucial.\n",
            "\n",
            "Overall, the Llama model is a powerful tool for natural language processing, with applications in conversational AI, text summarization, language translation, and content creation. While it's not perfect and may require human oversight and evaluation, it's an exciting development in the field of AI research."
          ]
        }
      ],
      "source": [
        "# Test the basic chat...\n",
        "\n",
        "from langserve import RemoteRunnable\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = RemoteRunnable(\"http://0.0.0.0:9012/basic_chat/\") | StrOutputParser()\n",
        "for token in llm.stream(\"Tell me about the Llama model\"):\n",
        "    print(token, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0",
      "metadata": {
        "id": "5aff364c-519e-435e-bf1d-ce68a12d13e0"
      },
      "source": [
        "----\n",
        "\n",
        "<br>\n",
        "\n",
        "## <font color=\"#76b900\">**Congratulations On Completing The Course**</font>\n",
        "\n",
        "Hopefully this course was not only exciting and challenging, but also adequately prepared you for work on the cutting edge of LLM and RAG system development! Going forward, you should have the skills necessary to tackle industry-level challenges and explore RAG deployment with open-source models and frameworks.\n",
        "\n",
        "**Some NVIDIA-specific releases related to this that you may find interesting include:**\n",
        "- [**NVIDIA NIM**](https://www.nvidia.com/en-us/ai/), which offers microservice spinup routines that can be deployed on local compute.\n",
        "- [**TensorRT-LLM**](https://github.com/NVIDIA/TensorRT-LLM) is the current recommended framework for deploying GPU-accelerated LLM model engines in production settings.\n",
        "- [**NVIDIA's Generative AI Examples Repo**](https://github.com/NVIDIA/GenerativeAIExamples), which includes the current canonical microservice example application and will be updated with new resources as new production workflows get released.\n",
        "- [**The Knowledge-Based Chatbot Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief) which discusses additional publicly-accessible details on productionalizing RAG systems.\n",
        "\n",
        "**Additionally, some key topics you may be interested in delving more into include:**\n",
        "- [**LlamaIndex**](https://www.llamaindex.ai/), which has strong components that can augment and occasionally improve upon the LangChain RAG features.\n",
        "- [**LangSmith**](https://docs.smith.langchain.com/), an upcoming agent productionalization service offered by LangChain.\n",
        "- [**Gradio**](https://www.gradio.app/), though touched on in the course, has many more interface options which will be worth investigating. For inspiration, consider checking out [**HuggingFace Spaces**](https://huggingface.co/spaces) for examples.\n",
        "- [**LangGraph**](https://python.langchain.com/docs/langgraph/) is a framework for graph-based LLM orchestration, and is a natural next step forward for those interested in [multi-agent workflows](https://blog.langchain.dev/langgraph-multi-agent-workflows/).\n",
        "- [**DSPy**](https://github.com/stanfordnlp/dspy), a flow engineering framework that allows you to optimize LLM orchestration pipelines based on empirical performance results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035451c9-ed12-4bc3-b468-04db5c399e03",
      "metadata": {
        "id": "035451c9-ed12-4bc3-b468-04db5c399e03"
      },
      "source": [
        "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}